# YOLOv11 multimodal fusion model configuration
model:
  name: yolov11_fusion
  num_classes: 4
  classes: ['person', 'rider', 'bicycle', 'car']
  img_size: 256  # Further reduced for faster training (can be increased to 320, 416 or 640 for better accuracy)
  anchors:
    - [[4, 5], [6, 11], [12, 9]]  # Scale 1 (32x32 for 256) - scaled from 640 anchors
    - [[11, 24], [24, 18], [23, 46]]  # Scale 2 (16x16 for 256)
    - [[46, 35], [62, 78], [149, 130]]  # Scale 3 (8x8 for 256)
  backbone:
    type: DualBackbone
    in_channels: 3
  neck:
    type: PANet
    in_channels: [256, 512, 1024]
  head:
    type: YOLOv11Head
    num_anchors: 3
    in_channels: [256, 512, 1024]
  fusion:
    type: concatenate
    reduction_layers: [256, 512, 1024]

loss:
  box_weight: 0.5  # Per paper: λ1 = 0.5 for bounding box regression
  obj_weight: 1.0  # Per paper: λ2 = 1.0 for objectness
  cls_weight: 1.0  # Per paper: λ3 = 1.0 for classification (was 0.5, updated to match paper)
  iou_type: ciou  # Options: iou, giou, diou, ciou

paths:
  pretrained_weights: ""  # Path to pretrained weights (e.g., checkpoints/yolov11_fusion_best.pt)
  output_dir: "checkpoints"