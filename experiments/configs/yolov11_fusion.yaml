# YOLOv11 multimodal fusion model configuration
model:
  name: yolov11_fusion
  num_classes: 4
  classes: ['person', 'rider', 'bicycle', 'car']
  img_size: 416  # Matches default.yaml - can be increased to 640 for better accuracy
  anchors:
    - [[7, 8], [10, 20], [21, 15]]  # Scale 1 (52x52 for 416) - scaled from 640 anchors [[10,13], [16,30], [33,23]]
    - [[20, 40], [40, 29], [38, 77]]  # Scale 2 (26x26 for 416) - scaled from [[30,61], [62,45], [59,119]]
    - [[75, 59], [101, 129], [242, 212]]  # Scale 3 (13x13 for 416) - scaled from [[116,90], [156,198], [373,326]]
  backbone:
    type: DualBackbone
    in_channels: 3
  neck:
    type: PANet
    in_channels: [256, 512, 1024]
  head:
    type: YOLOv11Head
    num_anchors: 3
    in_channels: [256, 512, 1024]
  fusion:
    type: concatenate
    reduction_layers: [256, 512, 1024]

loss:
  box_weight: 0.5  # Per paper: λ1 = 0.5 for bounding box regression
  obj_weight: 1.0  # Per paper: λ2 = 1.0 for objectness
  cls_weight: 1.0  # Per paper: λ3 = 1.0 for classification (was 0.5, updated to match paper)
  iou_type: ciou  # Options: iou, giou, diou, ciou

paths:
  pretrained_weights: ""  # Path to pretrained weights (e.g., checkpoints/yolov11_fusion_best.pt)
  output_dir: "checkpoints"